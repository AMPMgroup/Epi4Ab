# Code version: 2025-05-10
# revision
# ===== model define =================================================================
# GNNNaive: v1.0.1 (onehot Ag) => always use pretrained with weight is 1. 
#                           => if use_struct, data is concatenated.
# GNNNaive: v1.0.2 (replaced onehot Ag w tokens) 
# GNNResNet: add-ons initial data into each layer
# added AntiBERTY for H3
# ====================================================================================
#
# === GNNnaive ===
model_name=GNNNaive
model_block=GNNNaiveBlock_Cheb
feature_version=v1.0.2
use_struct
use_pretrained
use_antiberty
use_token
<<<<<<< HEAD
#
# === BASE model ===
#use_base_model
#
# === common params ===
## -if below is disabled, protBERT is used as default.
#pretrained_model=ESM2_t12
use_relaxed
edge_type=dist
edge_threshold=10
label_from_ellipro_pi
block_norm=batchnorm
#block_norm=graphnorm
# ---
## == nodes/edges ==
### below are potential factors eg bond (A1), LJ repulse/attract (A2), and Coulombic (A3)
### activate any of these 3 to neglect the corresponding factor
#attribute_no_bond
#attribute_no_lj
#attribute_no_charge
#
### train weights for these 3 factors
#gradient_attribute
#
### weights after trained
#attribute_weight=0.23664245009422302 -0.0974310114979744 0.005594527814537287
attribute_weight=0.1735178381204605 -0.05394153669476509 -0.0020570196211338043
#
# ===== evaluate =====
num_layers=5
drop_out=0 0 0 0 0
hidden_channel=64 32 16 8 8 
num_kfold=3
filter_size=5
epoch_number=200
batch_size=50
loss_function=cross_entropy
cross_entropy_weight=0.5 1.0 1.0
attention_head=5
#
optimizer_method=adam
learning_rate=0.0005
weight_decay=1e-4
momentum=0.9
#
#===== final training =====
#train_all=with_validation
#train_all=yes
plot_network
=======
use_alphafold
#### Default: freeze_pretrained is True --> no need in parameters.txt
>>>>>>> d9e1fab
